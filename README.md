## KoGPT2 (í•œêµ­ì–´ GPT-2) Ver 2.0

`GPT-2`ëŠ” ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµëœ ì–¸ì–´ëª¨ë¸ì´ë©° ë¬¸ì¥ ìƒì„±ì— ìµœì í™” ë˜ì–´ ìˆìŠµë‹ˆë‹¤. `KoGPT2`ëŠ” ë¶€ì¡±í•œ í•œêµ­ì–´ ì„±ëŠ¥ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´ 40GB ì´ìƒì˜ í…ìŠ¤íŠ¸ë¡œ í•™ìŠµëœ í•œêµ­ì–´ ë””ì½”ë”(`decoder`) ì–¸ì–´ëª¨ë¸ì…ë‹ˆë‹¤. 


### How to install

```
pip install git+https://github.com/SKT-AI/KoGPT2#egg=kogpt2
```


### Tokenizer


[`tokenizers`](https://github.com/huggingface/tokenizers) íŒ¨í‚¤ì§€ì˜ `Character BPE tokenizer`ë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤. 

ì‚¬ì „ í¬ê¸°ëŠ” 51,200 ì´ë©° ëŒ€í™”ì— ìì£¼ ì“°ì´ëŠ” ì•„ë˜ì™€ ê°™ì€ ì´ëª¨í‹°ì½˜, ì´ëª¨ì§€ ë“±ì„ ì¶”ê°€í•˜ì—¬ í•´ë‹¹ í† í°ì˜ ì¸ì‹ ëŠ¥ë ¥ì„ ì˜¬ë ¸ìŠµë‹ˆë‹¤. 
> ğŸ˜€, ğŸ˜, ğŸ˜†, ğŸ˜…, ğŸ¤£, .. , `:-)`, `:)`, `-)`, `(-:`...

ë˜í•œ `<unused0>` ~ `<unused99>`ë“±ì˜ ë¯¸ì‚¬ìš© í† í°ì„ ì •ì˜í•´ í•„ìš”í•œ í…ŒìŠ¤í¬ì— ë”°ë¼ ììœ ë¡­ê²Œ ì •ì˜í•´ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í–ˆìŠµë‹ˆë‹¤.

```python
from transformers import PreTrainedTokenizerFast
tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>', use_auth_token=True) 
tokenizer.tokenize("ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o")
['â–ì•ˆë…•', 'í•˜', 'ì„¸', 'ìš”.', 'â–í•œêµ­ì–´', 'â–G', 'P', 'T', '-2', 'â–ì…', 'ë‹ˆë‹¤.', 'ğŸ˜¤', ':)', 'l^o']
```

### Model

| Model       |  # of params |   Type   | # of layers  | # of heads | ffn_dim | hidden_dims | 
|--------------|:----:|:-------:|--------:|--------:|--------:|--------------:|
| `KoGPT2` |  125M  |  Decoder |   12     | 12      | 3072    | 768 | 


```python
import torch
from transformers import GPT2LMHeadModel

model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', use_auth_token=True)
text = 'í˜„ëŒ€ì¸ë“¤ì€ ì™œ í•­ìƒ ë¶ˆì•ˆí•´ í• ê¹Œ?'
input_ids = tokenizer.encode(text)
gen_ids = model.generate(torch.tensor([input_ids]),
                            max_length=128,
                            repetition_penalty=2.0,
                            use_cache=True,
                            pad_token_id=tokenizer.pad_token_id,
                            eos_token_id=tokenizer.eos_token_id,
                            bos_token_id=tokenizer.bos_token_id)
generated = tokenizer.decode(gen_ids[0,:].tolist())
print(generated)
```

#### Performances

#### Classification or Regression

|   |  [NSMC](https://github.com/e9t/nsmc)(acc)  | [KorSTS](https://github.com/kakaobrain/KorNLUDatasets)(spearman) | PPL | 
|---|---|---|---|
| **KoGPT 2.0**  | 93.3  | 78.4  | 24.6  |
| KoGPT 1.0  | 89.9  | 80.1  | 45.4  |


### Data

í•œêµ­ì–´ ìœ„í‚¤ ë°±ê³¼ ì´ì™¸, ë‰´ìŠ¤, [ëª¨ë‘ì˜ ë§ë­‰ì¹˜ v1.0](https://corpus.korean.go.kr/), [ì²­ì™€ëŒ€ êµ­ë¯¼ì²­ì›](https://github.com/akngs/petitions) ë“±ì˜ ë‹¤ì–‘í•œ ë°ì´í„°ê°€ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.



### Demo

[ë°ëª¨ ë§í¬](http://52.231.69.211:8080/)

<table><tr><td>
    <center><img src="imgs/kogpt2_2.png" width="452"/></center>
</td></tr>
</table>

### Training

*ì•„ë˜ ë‚´ìš©ì€ KoGPT2 ver 1.0 í•™ìŠµ ì•„í‚¤í…ì²˜ì´ë©° 2.0ê³¼ëŠ” ë‹¤ë¦„*

<table><tr><td>
    <center><img src="imgs/KoGPT2Traning-horovod.png" width="452"/></center>
</td></tr>
</table>


- ì´ ì‘ì—…ì€ ì•„ë˜ ì„¸ íŒ€ê³¼ì˜ í˜‘ì—…ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.
  - `SKT Conv.AI`íŒ€ : ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸ í•™ìŠµ ë¡œì§ êµ¬í˜„
  - `Amazon Machine Learning Solutions Lab`íŒ€ : ëŒ€ê·œëª¨ ë¶„ì‚° í•™ìŠµ ì¸í”„ë¼ êµ¬ì„±
  - [`GluonNLP`](https://gluon-nlp.mxnet.io/)íŒ€ : í•™ìŠµ í¼í¬ë¨¼ìŠ¤ ê°œì„ 

### Contacts

`KoGPT2` ê´€ë ¨ ì´ìŠˆëŠ” [ì´ê³³](https://github.com/SKT-AI/KoGPT2/issues)ì— ì˜¬ë ¤ì£¼ì„¸ìš”.

### License

`KoGPT2`ëŠ” `CC-BY-NC-SA 4.0` ë¼ì´ì„ ìŠ¤ í•˜ì— ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ ë° ì½”ë“œë¥¼ ì‚¬ìš©í•  ê²½ìš° ë¼ì´ì„ ìŠ¤ ë‚´ìš©ì„ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”. ë¼ì´ì„ ìŠ¤ ì „ë¬¸ì€ `LICENSE` íŒŒì¼ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
