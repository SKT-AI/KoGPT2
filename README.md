
<p>
<a href="https://console.tiyaro.ai/explore/skt-kogpt2-base-v2"> <img src="https://tiyaro-public-docs.s3.us-west-2.amazonaws.com/assets/try_on_tiyaro_badge.svg"></a>
</p>

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

* [KoGPT2 (í•œêµ­ì–´ GPT-2) Ver 2.0](#kogpt2-í•œêµ­ì–´-gpt-2-ver-20)
  * [Tokenizer](#tokenizer)
  * [Model](#model)
    * [Performances](#performances)
    * [Classification or Regression](#classification-or-regression)
  * [Data](#data)
  * [Demo](#demo)
  * [User Contributed Examples](#user-contributed-examples)
  * [Related press releases](#related-press-releases)
  * [Contacts](#contacts)
  * [License](#license)

<!-- /code_chunk_output -->


## KoGPT2 (í•œêµ­ì–´ GPT-2) Ver 2.0

[GPT-2](https://openai.com/blog/better-language-models/)ëŠ” ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµëœ ì–¸ì–´ëª¨ë¸ì´ë©° ë¬¸ì¥ ìƒì„±ì— ìµœì í™” ë˜ì–´ ìˆìŠµë‹ˆë‹¤. `KoGPT2`ëŠ” ë¶€ì¡±í•œ í•œêµ­ì–´ ì„±ëŠ¥ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´ 40GB ì´ìƒì˜ í…ìŠ¤íŠ¸ë¡œ í•™ìŠµëœ í•œêµ­ì–´ ë””ì½”ë”(`decoder`) ì–¸ì–´ëª¨ë¸ì…ë‹ˆë‹¤.

<table><tr><td>
    <center><img src="imgs/gpt2.png" width="452"/></center>
</td></tr>
</table>



### Tokenizer


[`tokenizers`](https://github.com/huggingface/tokenizers) íŒ¨í‚¤ì§€ì˜ `Character BPE tokenizer`ë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.

ì‚¬ì „ í¬ê¸°ëŠ” 51,200 ì´ë©° ëŒ€í™”ì— ìì£¼ ì“°ì´ëŠ” ì•„ë˜ì™€ ê°™ì€ ì´ëª¨í‹°ì½˜, ì´ëª¨ì§€ ë“±ì„ ì¶”ê°€í•˜ì—¬ í•´ë‹¹ í† í°ì˜ ì¸ì‹ ëŠ¥ë ¥ì„ ì˜¬ë ¸ìŠµë‹ˆë‹¤.
> ğŸ˜€, ğŸ˜, ğŸ˜†, ğŸ˜…, ğŸ¤£, .. , `:-)`, `:)`, `-)`, `(-:`...

ë˜í•œ `<unused0>` ~ `<unused99>`ë“±ì˜ ë¯¸ì‚¬ìš© í† í°ì„ ì •ì˜í•´ í•„ìš”í•œ í…ŒìŠ¤í¬ì— ë”°ë¼ ììœ ë¡­ê²Œ ì •ì˜í•´ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í–ˆìŠµë‹ˆë‹¤.

```python
> from transformers import PreTrainedTokenizerFast
> tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2",
  bos_token='</s>', eos_token='</s>', unk_token='<unk>',
  pad_token='<pad>', mask_token='<mask>')
> tokenizer.tokenize("ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o")
['â–ì•ˆë…•', 'í•˜', 'ì„¸', 'ìš”.', 'â–í•œêµ­ì–´', 'â–G', 'P', 'T', '-2', 'â–ì…', 'ë‹ˆë‹¤.', 'ğŸ˜¤', ':)', 'l^o']
```

### Model

| Model       |  # of params |   Type   | # of layers  | # of heads | ffn_dim | hidden_dims |
|--------------|:----:|:-------:|--------:|--------:|--------:|--------------:|
| `kogpt2-base-v2` |  125M  |  Decoder |   12     | 12      | 3072    | 768 |


```python
> import torch
> from transformers import GPT2LMHeadModel

> model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')
> text = 'ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ”'
> input_ids = tokenizer.encode(text, return_tensors='pt')
> gen_ids = model.generate(input_ids,
                           max_length=128,
                           repetition_penalty=2.0,
                           pad_token_id=tokenizer.pad_token_id,
                           eos_token_id=tokenizer.eos_token_id,
                           bos_token_id=tokenizer.bos_token_id,
                           use_cache=True)
> generated = tokenizer.decode(gen_ids[0])
> print(generated)
ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ” ë¬´ì—‡ë³´ë‹¤ ê·œì¹™ì ì¸ ìƒí™œìŠµê´€ì´ ì¤‘ìš”í•˜ë‹¤.
íŠ¹íˆ, ì•„ì¹¨ì‹ì‚¬ëŠ” ë‹¨ë°±ì§ˆê³¼ ë¹„íƒ€ë¯¼ì´ í’ë¶€í•œ ê³¼ì¼ê³¼ ì±„ì†Œë¥¼ ë§ì´ ì„­ì·¨í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.
ë˜í•œ í•˜ë£¨ 30ë¶„ ì´ìƒ ì¶©ë¶„í•œ ìˆ˜ë©´ì„ ì·¨í•˜ëŠ” ê²ƒë„ ë„ì›€ì´ ëœë‹¤.
ì•„ì¹¨ ì‹ì‚¬ë¥¼ ê±°ë¥´ì§€ ì•Šê³  ê·œì¹™ì ìœ¼ë¡œ ìš´ë™ì„ í•˜ë©´ í˜ˆì•¡ìˆœí™˜ì— ë„ì›€ì„ ì¤„ ë¿ë§Œ ì•„ë‹ˆë¼ ì‹ ì§„ëŒ€ì‚¬ë¥¼ ì´‰ì§„í•´ ì²´ë‚´ ë…¸íë¬¼ì„ ë°°ì¶œí•˜ê³  í˜ˆì••ì„ ë‚®ì¶°ì¤€ë‹¤.
ìš´ë™ì€ í•˜ë£¨ì— 10ë¶„ ì •ë„ë§Œ í•˜ëŠ” ê²Œ ì¢‹ìœ¼ë©° ìš´ë™ í›„ì—ëŠ” ë°˜ë“œì‹œ ìŠ¤íŠ¸ë ˆì¹­ì„ í†µí•´ ê·¼ìœ¡ëŸ‰ì„ ëŠ˜ë¦¬ê³  ìœ ì—°ì„±ì„ ë†’ì—¬ì•¼ í•œë‹¤.
ìš´ë™ í›„ ë°”ë¡œ ì ìë¦¬ì— ë“œëŠ” ê²ƒì€ í”¼í•´ì•¼ í•˜ë©° íŠ¹íˆ ì•„ì¹¨ì— ì¼ì–´ë‚˜ë©´ ëª¸ì´ í”¼ê³¤í•´ì§€ê¸° ë•Œë¬¸ì— ë¬´ë¦¬í•˜ê²Œ ì›€ì§ì´ë©´ ì˜¤íˆë ¤ ì—­íš¨ê³¼ê°€ ë‚  ìˆ˜ë„ ìˆë‹¤...
```

#### Performances

#### Classification or Regression

|   |  [NSMC](https://github.com/e9t/nsmc)(acc)  | [KorSTS](https://github.com/kakaobrain/KorNLUDatasets)(spearman) |
|---|---|---|
| **KoGPT2 2.0**  | 89.1  | 77.8  |


### Data

[í•œêµ­ì–´ ìœ„í‚¤ ë°±ê³¼](https://ko.wikipedia.org/) ì´ì™¸, ë‰´ìŠ¤, [ëª¨ë‘ì˜ ë§ë­‰ì¹˜ v1.0](https://corpus.korean.go.kr/), [ì²­ì™€ëŒ€ êµ­ë¯¼ì²­ì›](https://github.com/akngs/petitions) ë“±ì˜ ë‹¤ì–‘í•œ ë°ì´í„°ê°€ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.



### Demo

[ë°ëª¨ ë§í¬](https://huggingface.co/spaces/gogamza/kogpt2-base-v2)

<table><tr><td>
    <center><img src="imgs/kogpt2_2.png" width="452"/></center>
</td></tr>
</table>


### User Contributed Examples
- [êµ­ë¦½êµ­ì–´ì› ì‹ ë¬¸ ë§ë­‰ì¹˜(Ver.1.1) ë‰´ìŠ¤ê¸°ì‚¬ í† í”½ ë¶„ë¥˜](https://github.com/seawavve/newsTopicClassification)

### Related press releases

- [SKT, ê¸€ ì“°ëŠ” AI 'KoGPT2' ìƒˆ ë²„ì „ ê°œë°œâ€¦ë¬¸ì¥â†’ë¬¸ë‹¨ìƒì„±ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ](https://www.ajunews.com/view/20210504120317549)
- [[AI ëª¨ë¸ íƒí—˜ê¸°] #7 í•œê¸€ ë²„ì „ì˜ GPT-2, KoGPT2](https://medium.com/ai-networkkr/ai-%EB%AA%A8%EB%8D%B8-%ED%83%90%ED%97%98%EA%B8%B0-7-%ED%95%9C%EA%B8%80-%EB%B2%84%EC%A0%84%EC%9D%98-gpt-2-f7317e6499f9)

### Contacts

`KoGPT2` ê´€ë ¨ ì´ìŠˆëŠ” [ì´ê³³](https://github.com/SKT-AI/KoGPT2/issues)ì— ì˜¬ë ¤ì£¼ì„¸ìš”.


### License

`KoGPT2`ëŠ” [CC-BY-NC-SA 4.0 ë¼ì´ì„ ìŠ¤](https://creativecommons.org/licenses/by-nc-sa/4.0/) í•˜ì— ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ ë° ì½”ë“œë¥¼ ì‚¬ìš©í•  ê²½ìš° ë¼ì´ì„ ìŠ¤ ë‚´ìš©ì„ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”. ë¼ì´ì„ ìŠ¤ ì „ë¬¸ì€ [LICENSE](https://github.com/SKT-AI/KoGPT2/blob/master/LICENSE) íŒŒì¼ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
